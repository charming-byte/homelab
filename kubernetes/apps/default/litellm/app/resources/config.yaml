model_list:
  - model_name: github_copilot/gpt-5-mini
    litellm_params:
      model: github_copilot/gpt-5-mini

  - model_name: github_copilot/gpt-5
    litellm_params:
      model: github_copilot/gpt-5

  - model_name: github_copilot/gpt-4o-mini-2024-07-18
    litellm_params:
      model: github_copilot/gpt-4o-mini-2024-07-18

  - model_name: github_copilot/gpt-4o-2024-11-20
    litellm_params:
      model: github_copilot/gpt-4o-2024-11-20

  - model_name: github_copilot/gpt-4o-2024-08-06
    litellm_params:
      model: github_copilot/gpt-4o-2024-08-06

  - model_name: github_copilot/grok-code-fast-1
    litellm_params:
      model: github_copilot/grok-code-fast-1

  - model_name: github_copilot/gpt-5.1
    litellm_params:
      model: github_copilot/gpt-5.1

  - model_name: github_copilot/gpt-5.1-codex
    model_info:
      mode: responses
    litellm_params:
      model: github_copilot/gpt-5.1-codex

  - model_name: github_copilot/gpt-5.1-codex-mini
    model_info:
      mode: responses
    litellm_params:
      model: github_copilot/gpt-5.1-codex-mini

  - model_name: github_copilot/gpt-5.1-codex-max
    model_info:
      mode: responses
    litellm_params:
      model: github_copilot/gpt-5.1-codex-max

  - model_name: github_copilot/gpt-5-codex
    model_info:
      mode: responses
    litellm_params:
      model: github_copilot/gpt-5-codex

  - model_name: github_copilot/text-embedding-3-small
    model_info:
      mode: embedding
    litellm_params:
      model: github_copilot/text-embedding-3-small

  - model_name: github_copilot/text-embedding-3-small-inference
    model_info:
      mode: embedding
    litellm_params:
      model: github_copilot/text-embedding-3-small-inference

  - model_name: github_copilot/claude-sonnet-4
    litellm_params:
      model: github_copilot/claude-sonnet-4

  - model_name: github_copilot/claude-sonnet-4.5
    litellm_params:
      model: github_copilot/claude-sonnet-4.5

  - model_name: github_copilot/claude-opus-4.5
    litellm_params:
      model: github_copilot/claude-opus-4.5

  - model_name: github_copilot/claude-haiku-4.5
    litellm_params:
      model: github_copilot/claude-haiku-4.5

  - model_name: github_copilot/gemini-3-pro-preview
    litellm_params:
      model: github_copilot/gemini-3-pro-preview

  - model_name: github_copilot/gemini-3-flash-preview
    litellm_params:
      model: github_copilot/gemini-3-flash-preview

  - model_name: github_copilot/gemini-2.5-pro
    litellm_params:
      model: github_copilot/gemini-2.5-pro

  - model_name: github_copilot/gpt-4.1-2025-04-14
    litellm_params:
      model: github_copilot/gpt-4.1-2025-04-14

  - model_name: github_copilot/oswe-vscode-prime
    litellm_params:
      model: github_copilot/oswe-vscode-prime

  - model_name: github_copilot/oswe-vscode-secondary
    litellm_params:
      model: github_copilot/oswe-vscode-secondary

  - model_name: github_copilot/gpt-5.2
    litellm_params:
      model: github_copilot/gpt-5.2

  - model_name: github_copilot/gpt-41-copilot
    litellm_params:
      model: github_copilot/gpt-41-copilot

  - model_name: github_copilot/gpt-3.5-turbo-0613
    litellm_params:
      model: github_copilot/gpt-3.5-turbo-0613

  - model_name: github_copilot/gpt-4
    litellm_params:
      model: github_copilot/gpt-4

  - model_name: github_copilot/gpt-4-0613
    litellm_params:
      model: github_copilot/gpt-4-0613

  - model_name: github_copilot/gpt-4-0125-preview
    litellm_params:
      model: github_copilot/gpt-4-0125-preview

  - model_name: github_copilot/gpt-4o-2024-05-13
    litellm_params:
      model: github_copilot/gpt-4o-2024-05-13

  - model_name: github_copilot/gpt-4-o-preview
    litellm_params:
      model: github_copilot/gpt-4-o-preview

  - model_name: github_copilot/gpt-4.1
    litellm_params:
      model: github_copilot/gpt-4.1

  - model_name: github_copilot/gpt-3.5-turbo
    litellm_params:
      model: github_copilot/gpt-3.5-turbo

  - model_name: github_copilot/gpt-4o-mini
    litellm_params:
      model: github_copilot/gpt-4o-mini

  - model_name: github_copilot/gpt-4o
    litellm_params:
      model: github_copilot/gpt-4o

  - model_name: github_copilot/text-embedding-ada-002
    model_info:
      mode: embedding
    litellm_params:
      model: github_copilot/text-embedding-ada-002

general_settings:
  proxy_batch_write_at: 60 # Batch write spend updates every 60s
  database_connection_pool_limit: 10 # limit the number of database connections to = MAX Number of DB Connections/Number of instances of litellm proxy (Around 10-20 is good number)
  use_redis_transaction_buffer: true # HA

  # OPTIONAL Best Practices
  disable_error_logs: True # turn off writing LLM Exceptions to DB
  allow_requests_on_db_unavailable: True # Only USE when running LiteLLM on your VPC. Allow requests to still be processed even if the DB is unavailable. We recommend doing this if you're running LiteLLM on VPC that cannot be accessed from the public internet.

router_settings:
  routing_strategy: usage-based-routing-v2
  redis_host: os.environ/REDIS_HOST

litellm_settings:
  #  request_timeout: 600    # raise Timeout error if call takes longer than 600 seconds. Default value is 6000seconds if not set
  set_verbose: False # Switch off Debug Logging, ensure your logs do not have any debugging on
  json_logs: true # Get debug logs in json format
  cache: True
  cache_params:
    type: redis
